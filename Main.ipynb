{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c44d416d",
   "metadata": {},
   "source": [
    "Step 1: Setup and Data Download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00823dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notebook-cell version: VGG19 transfer learning on Oxford 102 Flowers\n",
    "- Random split 50/25/25\n",
    "- Run twice by calling run_experiment(split_seed=1) and run_experiment(split_seed=2)\n",
    "- Saves curves + checkpoint into out_dir\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset, Subset\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import scipy.io as sio\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "268fc0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Flowers102Local(Dataset):\n",
    "    \"\"\"\n",
    "    Local Oxford 102 Flowers dataset:\n",
    "    root/\n",
    "      jpg/\n",
    "      imagelabels.mat\n",
    "      setid.mat\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.img_dir = self.root / \"jpg\"\n",
    "        self.transform = transform\n",
    "\n",
    "        mat = sio.loadmat(self.root / \"imagelabels.mat\")\n",
    "        labels = mat[\"labels\"].squeeze().astype(int)  # 1..102\n",
    "        self.labels = (labels - 1).tolist()           # 0..101\n",
    "\n",
    "        self.image_paths = [\n",
    "            self.img_dir / f\"image_{i:05d}.jpg\"\n",
    "            for i in range(1, len(self.labels) + 1)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        y = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6b8873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Dataset utilities\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class SplitIndices:\n",
    "    train: List[int]\n",
    "    val: List[int]\n",
    "    test: List[int]\n",
    "\n",
    "\n",
    "class TransformOverrideDataset(Dataset):\n",
    "    def __init__(self, base: Dataset, transform):\n",
    "        self.base = base\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.base[idx]\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e65cddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flowers102_all(root: str) -> Dataset:\n",
    "    return Flowers102Local(root=root, transform=None)\n",
    "\n",
    "\n",
    "def make_random_split_indices(n: int, split_seed: int) -> SplitIndices:\n",
    "    g = torch.Generator().manual_seed(split_seed)\n",
    "    perm = torch.randperm(n, generator=g).tolist()\n",
    "\n",
    "    n_train = int(0.50 * n)\n",
    "    n_val = int(0.25 * n)\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train_idx = perm[:n_train]\n",
    "    val_idx = perm[n_train:n_train + n_val]\n",
    "    test_idx = perm[n_train + n_val:n_train + n_val + n_test]\n",
    "\n",
    "    return SplitIndices(train=train_idx, val=val_idx, test=test_idx)\n",
    "\n",
    "\n",
    "def save_split_indices(path: str, split: SplitIndices) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"train\": split.train, \"val\": split.val, \"test\": split.test}, f)\n",
    "\n",
    "\n",
    "def load_split_indices(path: str) -> SplitIndices:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        obj = json.load(f)\n",
    "    return SplitIndices(train=obj[\"train\"], val=obj[\"val\"], test=obj[\"test\"])\n",
    "\n",
    "\n",
    "def build_transforms(img_size: int = 224):\n",
    "    imagenet_mean = [0.485, 0.456, 0.406]\n",
    "    imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.02),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "    ])\n",
    "\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "    ])\n",
    "\n",
    "    return train_tf, eval_tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "681b4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(\n",
    "    root: str,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    split_seed: int,\n",
    "    split_cache_path: str,\n",
    "    img_size: int = 224,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader, SplitIndices]:\n",
    "    base_all = load_flowers102_all(root=root)\n",
    "    n = len(base_all)\n",
    "\n",
    "    if os.path.isfile(split_cache_path):\n",
    "        split = load_split_indices(split_cache_path)\n",
    "    else:\n",
    "        split = make_random_split_indices(n=n, split_seed=split_seed)\n",
    "        save_split_indices(split_cache_path, split)\n",
    "\n",
    "    train_tf, eval_tf = build_transforms(img_size=img_size)\n",
    "\n",
    "    train_ds = TransformOverrideDataset(Subset(base_all, split.train), transform=train_tf)\n",
    "    val_ds   = TransformOverrideDataset(Subset(base_all, split.val), transform=eval_tf)\n",
    "    test_ds  = TransformOverrideDataset(Subset(base_all, split.test), transform=eval_tf)\n",
    "\n",
    "    # In Windows notebooks, num_workers>0 can cause issues. Use 0 unless you know it's stable.\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76ae9df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Model\n",
    "# -----------------------------\n",
    "def build_vgg19_classifier(num_classes: int = 102, freeze_features: bool = True) -> nn.Module:\n",
    "    model = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    if freeze_features:\n",
    "        for p in model.features.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    in_features = model.classifier[-1].in_features\n",
    "    model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a9089ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Train/Eval loops\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device, criterion: nn.Module) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    return total_loss / max(1, total), correct / max(1, total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67efd1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    ") -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    return total_loss / max(1, total), correct / max(1, total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "568d4d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_curves(out_dir: str, history: Dict[str, List[float]]) -> None:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history[\"train_acc\"], label=\"train\")\n",
    "    plt.plot(history[\"val_acc\"], label=\"val\")\n",
    "    plt.plot(history[\"test_acc\"], label=\"test\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"accuracy_vgg19.png\"), dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history[\"train_loss\"], label=\"train\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"val\")\n",
    "    plt.plot(history[\"test_loss\"], label=\"test\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"cross_entropy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"loss_vgg19.png\"), dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "    with open(os.path.join(out_dir, \"history_vgg19.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(history, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8154c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(path: str, model: nn.Module, optimizer: optim.Optimizer, epoch: int, best_val_acc: float) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(\n",
    "        {\"epoch\": epoch, \"model_state\": model.state_dict(),\n",
    "         \"optimizer_state\": optimizer.state_dict(), \"best_val_acc\": best_val_acc},\n",
    "        path,\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_proba(model: nn.Module, images: torch.Tensor, device: torch.device) -> torch.Tensor:\n",
    "    model.eval()\n",
    "    images = images.to(device)\n",
    "    logits = model(images)\n",
    "    return torch.softmax(logits, dim=1).cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6481fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "#  \"main\"\n",
    "# -----------------------------\n",
    "def run_experiment(\n",
    "    data_root: str = \"./data\",\n",
    "    out_dir: str = \"./results/vgg19_seed1\",\n",
    "    epochs: int = 35,\n",
    "    batch_size: int = 32,\n",
    "    lr: float = 1e-4,\n",
    "    weight_decay: float = 0.0,\n",
    "    num_workers: int = 0,         # NOTE: 0 is safest in Windows notebooks\n",
    "    img_size: int = 224,\n",
    "    split_seed: int = 1,\n",
    "    freeze_features: bool = True,\n",
    "    early_stop_patience: int = 7,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA requested but not available. Falling back to CPU.\")\n",
    "        device = \"cpu\"\n",
    "    device_t = torch.device(device)\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    split_cache_path = os.path.join(out_dir, f\"split_indices_seed_{split_seed}.json\")\n",
    "\n",
    "    set_seed(split_seed)\n",
    "\n",
    "    train_loader, val_loader, test_loader, _split = make_loaders(\n",
    "        root=data_root,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        split_seed=split_seed,\n",
    "        split_cache_path=split_cache_path,\n",
    "        img_size=img_size,\n",
    "    )\n",
    "\n",
    "    model = build_vgg19_classifier(num_classes=102, freeze_features=freeze_features).to(device_t)\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(trainable_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    history = {k: [] for k in [\"train_loss\",\"val_loss\",\"test_loss\",\"train_acc\",\"val_acc\",\"test_acc\"]}\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    best_val_loss = math.inf\n",
    "    epochs_no_improve = 0\n",
    "    ckpt_path = os.path.join(out_dir, \"best_vgg19.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, device_t, criterion, optimizer)\n",
    "        va_loss, va_acc = evaluate(model, val_loader, device_t, criterion)\n",
    "        te_loss, te_acc = evaluate(model, test_loader, device_t, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"val_loss\"].append(va_loss)\n",
    "        history[\"test_loss\"].append(te_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_acc\"].append(va_acc)\n",
    "        history[\"test_acc\"].append(te_acc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d}/{epochs} | \"\n",
    "            f\"train loss {tr_loss:.4f} acc {tr_acc:.4f} | \"\n",
    "            f\"val loss {va_loss:.4f} acc {va_acc:.4f} | \"\n",
    "            f\"test loss {te_loss:.4f} acc {te_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        if va_acc > best_val_acc:\n",
    "            best_val_acc = va_acc\n",
    "            save_checkpoint(ckpt_path, model, optimizer, epoch, best_val_acc)\n",
    "\n",
    "        if va_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = va_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= early_stop_patience:\n",
    "                print(f\"Early stopping triggered (patience={early_stop_patience}).\")\n",
    "                break\n",
    "\n",
    "    save_curves(out_dir, history)\n",
    "\n",
    "    best_epoch = int(np.argmax(history[\"val_acc\"])) + 1\n",
    "    print(\"\\nSummary\")\n",
    "    print(f\"- Split seed: {split_seed}\")\n",
    "    print(f\"- Best val acc: {max(history['val_acc']):.4f} (epoch {best_epoch})\")\n",
    "    print(f\"- Last test acc: {history['test_acc'][-1]:.4f}\")\n",
    "    print(f\"- Saved: {os.path.join(out_dir, 'accuracy_vgg19.png')}\")\n",
    "    print(f\"- Saved: {os.path.join(out_dir, 'loss_vgg19.png')}\")\n",
    "    print(f\"- Checkpoint: {ckpt_path}\")\n",
    "    print(f\"- Split indices: {split_cache_path}\")\n",
    "\n",
    "    return history, ckpt_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76888dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Determinism (can reduce speed; )\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0bb88ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/35 | train loss 3.1448 acc 0.2865 | val loss 1.5717 acc 0.6116 | test loss 1.6156 acc 0.6099\n",
      "Epoch 002/35 | train loss 1.3604 acc 0.6412 | val loss 0.9499 acc 0.7538 | test loss 0.9626 acc 0.7480\n",
      "Epoch 003/35 | train loss 0.8579 acc 0.7587 | val loss 0.7738 acc 0.7831 | test loss 0.7844 acc 0.7871\n",
      "Epoch 004/35 | train loss 0.6324 acc 0.8190 | val loss 0.6457 acc 0.8217 | test loss 0.6614 acc 0.8193\n",
      "Epoch 005/35 | train loss 0.4731 acc 0.8625 | val loss 0.6027 acc 0.8334 | test loss 0.6209 acc 0.8330\n",
      "Epoch 006/35 | train loss 0.3598 acc 0.8898 | val loss 0.5875 acc 0.8315 | test loss 0.6244 acc 0.8384\n",
      "Epoch 007/35 | train loss 0.3036 acc 0.9108 | val loss 0.5879 acc 0.8344 | test loss 0.6061 acc 0.8364\n",
      "Epoch 008/35 | train loss 0.2374 acc 0.9287 | val loss 0.5396 acc 0.8520 | test loss 0.5580 acc 0.8540\n",
      "Epoch 009/35 | train loss 0.2295 acc 0.9304 | val loss 0.5612 acc 0.8447 | test loss 0.5477 acc 0.8579\n",
      "Epoch 010/35 | train loss 0.1890 acc 0.9436 | val loss 0.5072 acc 0.8500 | test loss 0.5317 acc 0.8521\n",
      "Epoch 011/35 | train loss 0.1515 acc 0.9555 | val loss 0.5295 acc 0.8549 | test loss 0.5339 acc 0.8657\n",
      "Epoch 012/35 | train loss 0.1302 acc 0.9617 | val loss 0.5341 acc 0.8539 | test loss 0.5575 acc 0.8599\n",
      "Epoch 013/35 | train loss 0.1241 acc 0.9619 | val loss 0.5429 acc 0.8544 | test loss 0.5619 acc 0.8540\n",
      "Epoch 014/35 | train loss 0.1116 acc 0.9651 | val loss 0.5485 acc 0.8534 | test loss 0.5832 acc 0.8491\n",
      "Epoch 015/35 | train loss 0.0910 acc 0.9736 | val loss 0.5320 acc 0.8622 | test loss 0.5332 acc 0.8608\n",
      "Epoch 016/35 | train loss 0.0937 acc 0.9722 | val loss 0.5345 acc 0.8578 | test loss 0.5502 acc 0.8662\n",
      "Epoch 017/35 | train loss 0.0690 acc 0.9805 | val loss 0.5832 acc 0.8608 | test loss 0.6090 acc 0.8491\n",
      "Early stopping triggered (patience=7).\n",
      "\n",
      "Summary\n",
      "- Split seed: 1\n",
      "- Best val acc: 0.8622 (epoch 15)\n",
      "- Last test acc: 0.8491\n",
      "- Saved: ./results/vgg19_seed1\\accuracy_vgg19.png\n",
      "- Saved: ./results/vgg19_seed1\\loss_vgg19.png\n",
      "- Checkpoint: ./results/vgg19_seed1\\best_vgg19.pt\n",
      "- Split indices: ./results/vgg19_seed1\\split_indices_seed_1.json\n"
     ]
    }
   ],
   "source": [
    "h1, ckpt1 = run_experiment(\n",
    "    data_root=r\"C:\\Users\\hp\\Downloads\\102flowers\",\n",
    "    split_seed=1,\n",
    "    out_dir=\"./results/vgg19_seed1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9634ef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/35 | train loss 3.1209 acc 0.2985 | val loss 1.5443 acc 0.6082 | test loss 1.4719 acc 0.6323\n",
      "Epoch 002/35 | train loss 1.3546 acc 0.6290 | val loss 0.9866 acc 0.7479 | test loss 0.9165 acc 0.7559\n",
      "Epoch 003/35 | train loss 0.8602 acc 0.7653 | val loss 0.8094 acc 0.7816 | test loss 0.7834 acc 0.7856\n",
      "Epoch 004/35 | train loss 0.6295 acc 0.8227 | val loss 0.7129 acc 0.8046 | test loss 0.6548 acc 0.8174\n",
      "Epoch 005/35 | train loss 0.4379 acc 0.8708 | val loss 0.6384 acc 0.8241 | test loss 0.5684 acc 0.8403\n",
      "Epoch 006/35 | train loss 0.3623 acc 0.8896 | val loss 0.6342 acc 0.8295 | test loss 0.5392 acc 0.8550\n",
      "Epoch 007/35 | train loss 0.3023 acc 0.9135 | val loss 0.6117 acc 0.8349 | test loss 0.5510 acc 0.8452\n",
      "Epoch 008/35 | train loss 0.2088 acc 0.9387 | val loss 0.5598 acc 0.8549 | test loss 0.4834 acc 0.8662\n",
      "Epoch 009/35 | train loss 0.2258 acc 0.9255 | val loss 0.6057 acc 0.8398 | test loss 0.5124 acc 0.8540\n",
      "Epoch 010/35 | train loss 0.1719 acc 0.9489 | val loss 0.5469 acc 0.8534 | test loss 0.4875 acc 0.8682\n",
      "Epoch 011/35 | train loss 0.1597 acc 0.9538 | val loss 0.5994 acc 0.8359 | test loss 0.5222 acc 0.8604\n",
      "Epoch 012/35 | train loss 0.1320 acc 0.9599 | val loss 0.6039 acc 0.8427 | test loss 0.5061 acc 0.8652\n",
      "Epoch 013/35 | train loss 0.1417 acc 0.9585 | val loss 0.5762 acc 0.8466 | test loss 0.4747 acc 0.8721\n",
      "Epoch 014/35 | train loss 0.1116 acc 0.9636 | val loss 0.5878 acc 0.8490 | test loss 0.4961 acc 0.8716\n",
      "Epoch 015/35 | train loss 0.0945 acc 0.9700 | val loss 0.5935 acc 0.8534 | test loss 0.5021 acc 0.8643\n",
      "Epoch 016/35 | train loss 0.0924 acc 0.9712 | val loss 0.6607 acc 0.8383 | test loss 0.5467 acc 0.8608\n",
      "Epoch 017/35 | train loss 0.0835 acc 0.9773 | val loss 0.6491 acc 0.8383 | test loss 0.5522 acc 0.8555\n",
      "Early stopping triggered (patience=7).\n",
      "\n",
      "Summary\n",
      "- Split seed: 2\n",
      "- Best val acc: 0.8549 (epoch 8)\n",
      "- Last test acc: 0.8555\n",
      "- Saved: ./results/vgg19_seed2\\accuracy_vgg19.png\n",
      "- Saved: ./results/vgg19_seed2\\loss_vgg19.png\n",
      "- Checkpoint: ./results/vgg19_seed2\\best_vgg19.pt\n",
      "- Split indices: ./results/vgg19_seed2\\split_indices_seed_2.json\n"
     ]
    }
   ],
   "source": [
    "h2, ckpt2 = run_experiment(\n",
    "    data_root=r\"C:\\Users\\hp\\Downloads\\102flowers\",\n",
    "    split_seed=2,\n",
    "    out_dir=\"./results/vgg19_seed2\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
